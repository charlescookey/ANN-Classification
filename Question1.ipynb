{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6a5c89-8c16-45ac-8225-4ca8c7cbb80e",
   "metadata": {},
   "source": [
    "## 0. Import Packages:\n",
    "First, we import all the packages we want to use in our implementation:\n",
    "* A library to use operating system dependent functionality\n",
    "* Package imaging library to deal with images in Python (PIL)\n",
    "* Package to find all paths which matches a specified pattern (glob)\n",
    "* Numpy Package (numpy)\n",
    "* PyTorch Framework (torch)\n",
    "* Neural Network Library of PyTorch (torch.nn)\n",
    "* PyTorch Optimisation Package (torch.optim)\n",
    "* PyTorch dataset loader package (torchvision.datasets)\n",
    "* PyTorch package for image preprocessing (torchvision.transforms)\n",
    "* A library to loop through the csv file (pandas)\n",
    "* A library for image manpulation (opencv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00075e7-8eb1-44dd-babc-74565ffa1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets as dsets\n",
    "from torchvision import transforms as trans\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5456bf-0b55-4590-8149-8b12ebbcb998",
   "metadata": {},
   "source": [
    "## 1. Crop Images:\n",
    "The idea is to generate a new dataset constituted of images generated from cropped images of the provided dataset. The cropping operation is defined by the bounding box and the label assigned to the new image by the label identifying the object in the uncropped image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d622ee9-675c-4b23-8457-8ba510f6eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Cropping 754 images\n"
     ]
    }
   ],
   "source": [
    "folder = 'dataset/images/'\n",
    "output_folder = 'dataset/cropped_images/'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv('dataset/labels.csv.csv')\n",
    "for index, row in df.iterrows():\n",
    "    curr_image_path = os.path.join(folder , row['image_name'])\n",
    "    curr_image = cv2.imread(curr_image_path)\n",
    "    if curr_image is None:\n",
    "        print(f\"Warning: {curr_image} not found. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Cropping an image\n",
    "    cropped_image = curr_image[ row['object_y1']: row['object_y2'] , row['object_x1']:row['object_x2']]\n",
    "    \n",
    "    # Save the cropped image\n",
    "    if cropped_image is None or cropped_image.size == 0:\n",
    "        print(f\"image {output_filename}: Empty\")\n",
    "        continue\n",
    "        \n",
    "    output_filename = os.path.join(output_folder , str(row['object_class'] - 1))#we want 0-3 , not 1-4\n",
    "    os.makedirs(output_filename, exist_ok=True)\n",
    "    \n",
    "    output_filename += '/' + str(index) + '.png' \n",
    "    writeStatus = cv2.imwrite(output_filename, cropped_image)\n",
    "    if writeStatus is False:\n",
    "        print(f\"Warning: {output_filename} not written\") # or raise exception, handle problem, etc.\n",
    "\n",
    "print(f'Finished Cropping {index + 1} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303bcb7d-9c53-4231-acec-2c14d2a1c610",
   "metadata": {},
   "source": [
    "## 2. Split dataset:\n",
    "The dataset is split into training, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9c9b9a-a3af-465a-bad9-0f74ca068ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For object class 0 we have 133 train 17 val and 17 test\n",
      "For object class 1 we have 144 train 18 val and 19 test\n",
      "For object class 2 we have 161 train 20 val and 21 test\n",
      "For object class 3 we have 163 train 20 val and 21 test\n",
      "In total we have 601 train 75 val and 78 test\n",
      "Altogether 754 files\n",
      "Dataset successfully split into train, validation, and test folders!\n"
     ]
    }
   ],
   "source": [
    "cropped_folder = 'dataset/cropped_images/'\n",
    "split_folder = 'dataset/split_dataset/' \n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(split_folder, split), exist_ok=True)\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "temp_ratio = val_ratio + test_ratio\n",
    "val_test_ratio = val_ratio / temp_ratio\n",
    "\n",
    "train_count =0\n",
    "test_count =0\n",
    "val_count =0\n",
    "\n",
    "for object_class in os.listdir(cropped_folder):\n",
    "    object_class_path = os.path.join(cropped_folder, object_class)\n",
    "\n",
    "    if not os.path.isdir(object_class_path):\n",
    "        continue \n",
    "    \n",
    "    images = [img for img in os.listdir(object_class_path) if img.endswith(('.png'))]\n",
    "\n",
    "    if len(images) == 0:\n",
    "        continue\n",
    "\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    train_files, temp_files = train_test_split(images, test_size=temp_ratio, random_state=7)\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=val_test_ratio, random_state=7)\n",
    "\n",
    "    def move_files(file_list, split):\n",
    "        curr_split_folder = os.path.join(split_folder, split, object_class)\n",
    "        os.makedirs(curr_split_folder, exist_ok=True)\n",
    "        for file in file_list:\n",
    "            shutil.move(os.path.join(object_class_path, file), os.path.join(curr_split_folder, file))\n",
    "\n",
    "    move_files(train_files, 'train')\n",
    "    move_files(val_files, 'val')\n",
    "    move_files(test_files, 'test')\n",
    "\n",
    "    print(f'For object class {object_class} we have {len(train_files)} train {len(val_files)} val and {len(test_files)} test')  \n",
    "    train_count += len(train_files)\n",
    "    test_count += len(val_files)\n",
    "    val_count += len(test_files)\n",
    "\n",
    "print(f'In total we have {train_count} train {test_count} val and {val_count} test')    \n",
    "print(f'Altogether {train_count + test_count + val_count} files')       \n",
    "print(\"Dataset successfully split into train, validation, and test folders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2adb4b-2c7f-4303-a0a0-038df124bcb2",
   "metadata": {},
   "source": [
    "## 3. Set Hyperparameters:\n",
    "Hyperparameters are settings that can be tuned to control the behaviour of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a38ac7-ebcc-4336-9dff-2031c1ee0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "image_size = 32*32\n",
    "num_classes = 4\n",
    "num_hidden_unit1 = 100\n",
    "num_hidden_unit2 = 50\n",
    "\n",
    "# Training Hyperparameters (Note: These values are not the optimal ones)\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "itr = 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197781e0-9965-4902-807b-a89c3741b4b2",
   "metadata": {},
   "source": [
    "* To run on gpu (not currently applicable in this training session), set cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6cb3cf-3727-466c-939b-23a32f31c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available() and cuda:\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    FloatType = torch.cuda.FloatTensor\n",
    "    LongType = torch.cuda.LongTensor\n",
    "else:\n",
    "    FloatType = torch.FloatTensor\n",
    "    LongType = torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213f11b-ccde-46f3-a396-1c1fd5508c16",
   "metadata": {},
   "source": [
    "## 4. Load Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66209be-556c-4d2d-baf9-2e7bc6265d87",
   "metadata": {},
   "source": [
    "* Define Transformation: Specify all the data preprocessing here in the order you want to apply them to the data\n",
    "    * Convert images to grayscale.\n",
    "    * Resize images to [32,32].\n",
    "    * Convert images to pytorch tensor.\n",
    "    * Normalise images with mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cce6cea-292c-4052-a1e7-70370826a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation\n",
    "transforms = trans.Compose([trans.Grayscale(), trans.Resize([32,32]), trans.ToTensor(), trans.Normalize(mean=(0.5,), std = (0.5,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b231a-0b40-4b58-9e21-fd8b77f96ebf",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82372cb1-0232-48d4-962e-764e486d9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 601, Validation Samples: 75, Test Samples: 78\n"
     ]
    }
   ],
   "source": [
    "split_folder = 'dataset/split_dataset/'\n",
    "class MIDSDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.paths = glob(os.path.join(self.root, '**', \"*.png\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = self.transform(Image.open(path))\n",
    "        label = int(path.split(os.path.sep)[-2])\n",
    "        return img, label\n",
    "\n",
    "# Create Subsets for PyTorch DataLoader\n",
    "train_dataset = MIDSDataset(root = os.path.join(split_folder, 'train'), transform= transforms)\n",
    "val_dataset = MIDSDataset(root = os.path.join(split_folder, 'val'), transform= transforms)\n",
    "test_dataset = MIDSDataset(root = os.path.join(split_folder, 'test'), transform= transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers = 0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 0)\n",
    "\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train Samples: {len(train_dataset)}, Validation Samples: {len(val_dataset)}, Test Samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1e737-b2de-4956-9f62-6e6d8825581a",
   "metadata": {},
   "source": [
    "## 3. Create Model:\n",
    "Here we want to define the two-layers neural network as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1376b015-8dbb-4069-b487-adc7aa3eca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, num_input=32*32, num_class=4, num_hidden_unit = 100):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.num_input = num_input\n",
    "        self.num_class = num_class\n",
    "        self.num_hidden_unit = num_hidden_unit\n",
    "        \n",
    "        # Defining the layers:\n",
    "        self.fc1 = nn.Linear(num_input, num_hidden_unit)\n",
    "        self.fc2 = nn.Linear(num_hidden_unit, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_input)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2ae3eb-a75e-4fac-9b9a-1d1d4ecd12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNN(nn.Module):\n",
    "    def __init__(self, num_input=32*32, num_class=4, num_hidden_unit1 = 100, num_hidden_unit2 = 50):\n",
    "        super(ThreeLayerNN, self).__init__()\n",
    "        self.num_input = num_input\n",
    "        self.num_class = num_class\n",
    "        self.num_hidden_unit1 = num_hidden_unit1\n",
    "        self.num_hidden_unit2 = num_hidden_unit2\n",
    "        \n",
    "        # Defining the layers:\n",
    "        self.fc1 = nn.Linear(num_input, num_hidden_unit)\n",
    "        self.fc2 = nn.Linear(num_hidden_unit1, num_hidden_unit2)\n",
    "        self.fc3 = nn.Linear(num_hidden_unit2, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_input)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ccafbee-82f9-450b-b1de-824d1b5f9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourLayerNN(nn.Module):\n",
    "    def __init__(self, num_input=32*32, num_class=4, num_hidden_unit1 = 100, num_hidden_unit2 = 50, num_hidden_unit3 = 25):\n",
    "        super(FourLayerNN, self).__init__()\n",
    "        self.num_input = num_input\n",
    "        self.num_class = num_class\n",
    "        self.num_hidden_unit1 = num_hidden_unit1\n",
    "        self.num_hidden_unit2 = num_hidden_unit2\n",
    "        self.num_hidden_unit3 = num_hidden_unit3\n",
    "        \n",
    "        # Defining the layers:\n",
    "        self.fc1 = nn.Linear(num_input, num_hidden_unit1)\n",
    "        self.fc2 = nn.Linear(num_hidden_unit1, num_hidden_unit2)\n",
    "        self.fc3 = nn.Linear(num_hidden_unit2, num_hidden_unit3)\n",
    "        self.fc4 = nn.Linear(num_hidden_unit3, num_class)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_input)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c5d37-c16e-4a11-9528-1d4f78d20156",
   "metadata": {},
   "source": [
    "## 4.  Write Learning Functions\n",
    "In this step, we define some functions that can be used for training/evaluation of image classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e71431-ac8b-4ae0-a70b-8ebbbb2cc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data)\n",
    "        m.bias.data.normal_(mean=0,std=1e-2)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data)\n",
    "        m.bias.data.normal_(mean=0,std=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7f63b-248d-464a-921c-fca1706934e6",
   "metadata": {},
   "source": [
    "### 4.1 Train Function\n",
    "Given the model, loss function, optimiser and data loader, this function can perform the training phase for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21dbbfdb-04f7-4ec7-abd3-049460b240d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, loss_func, epoch, vis_step = 5):\n",
    "    # Number of samples with correct classification\n",
    "    num_hit = 0\n",
    "    # total size of train data\n",
    "    total = len(train_loader.dataset)\n",
    "    # number of batch\n",
    "    num_batch = np.ceil(total/batch_size)\n",
    "    accumulative_loss = 0\n",
    "    # Training loop over batches of data on train dataset\n",
    "    for batch_idx, (image, labels) in enumerate(train_loader):\n",
    "        # 1. Clearing previous gradient values.\n",
    "        optimizer.zero_grad()\n",
    "        # 2. feeding images to model (forward method will be computed)\n",
    "        output = model(image)\n",
    "        # 3. Calculating the loss value\n",
    "        loss = loss_func(output, labels)\n",
    "        # 4. Calculating new grdients given the loss value\n",
    "        loss.backward()\n",
    "        # 5. Updating the weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss\n",
    "        accumulative_loss += loss.item()\n",
    "        # 6. logging (Optional)\n",
    "        if batch_idx % vis_step == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(image),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss.data.item()))\n",
    "    final_loss = accumulative_loss / num_batch\n",
    "    # Validation Phase on train dataset\n",
    "    for batch_idx, (image, labels) in enumerate(train_loader):\n",
    "        output = model(image)\n",
    "        _ , pred_label = output.data.max(dim=1)\n",
    "        num_hit += (pred_label == labels.data).sum()\n",
    "    train_accuracy = (num_hit.item() / total)\n",
    "    print(\"Epoch: {}, Training Accuracy: {:.2f}%\".format(epoch, 100. * train_accuracy))\n",
    "    return 100. * train_accuracy , final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c8e7b-4b95-452c-a3fc-129465685a4e",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation Function\n",
    "Given the model and data loader, this function can perform the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f057b04-9633-420c-b268-04faf0cd242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_val(model, val_loader, epoch):\n",
    "    num_hit = 0\n",
    "    total = len(test_loader.dataset)\n",
    "\n",
    "    for batch_idx, (image, labels) in enumerate(val_loader): # Complete the rest of this function\n",
    "        output = model(image)\n",
    "        _ , pred_label = output.data.max(dim=1)\n",
    "        num_hit += (pred_label == labels.data).sum()\n",
    "    test_accuracy = (num_hit.item() / total)\n",
    "    print(\"Epoch: {}, Validation Accuracy: {:.2f}%\".format(epoch, 100. * test_accuracy))\n",
    "    return 100. * test_accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aee92e3d-6a95-4666-aee1-1bfadb98468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_test(model, test_loader, epoch):\n",
    "    num_hit = 0\n",
    "    total = len(test_loader.dataset)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_idx, (image, labels) in enumerate(test_loader):\n",
    "        output = model(image)\n",
    "        _, pred_label = output.data.max(dim=1)\n",
    "        num_hit += (pred_label == labels.data).sum()\n",
    "        all_preds.extend(pred_label.cpu().numpy())\n",
    "        all_labels.extend(labels.data.cpu().numpy())\n",
    "\n",
    "    test_accuracy = (num_hit.item() / total)\n",
    "    print(\"Epoch: {}, Testing Accuracy: {:.2f}%\".format(epoch, 100. * test_accuracy))\n",
    "    return 100. * test_accuracy, all_preds, all_labels\n",
    "\n",
    "def final_evaluation(all_preds, all_labels):\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(\"Final Evaluation Metrics:\")\n",
    "    print(\"Precision: {:.2f}%, Recall: {:.2f}%, F1-Score: {:.2f}%\".format(precision*100., recall*100., f1*100.))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Analyze confusion matrix for class-specific performance\n",
    "    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "    for i, acc in enumerate(class_accuracy):\n",
    "        print(\"Class {} Accuracy: {:.2f}%\".format(i, 100. * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fc2cd-ebaf-4e67-9a14-6b74a75da1b8",
   "metadata": {},
   "source": [
    "## 5. Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cec674cf-341d-4b09-915c-00dd3d4d2c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/601 (0%)]\tLoss: 1.527480\n",
      "Train Epoch: 1 [80/601 (13%)]\tLoss: 1.404339\n",
      "Train Epoch: 1 [160/601 (26%)]\tLoss: 1.309817\n",
      "Train Epoch: 1 [240/601 (39%)]\tLoss: 1.410924\n",
      "Train Epoch: 1 [320/601 (53%)]\tLoss: 1.099898\n",
      "Train Epoch: 1 [400/601 (66%)]\tLoss: 1.137330\n",
      "Train Epoch: 1 [480/601 (79%)]\tLoss: 1.111842\n",
      "Train Epoch: 1 [560/601 (92%)]\tLoss: 1.139691\n",
      "Epoch: 1, Training Accuracy: 62.90%\n",
      "Epoch: 1, Validation Accuracy: 51.28%\n",
      "Epoch: 1, Testing Accuracy: 57.69%\n",
      "Train Epoch: 2 [0/601 (0%)]\tLoss: 0.980035\n",
      "Train Epoch: 2 [80/601 (13%)]\tLoss: 0.880570\n",
      "Train Epoch: 2 [160/601 (26%)]\tLoss: 0.985499\n",
      "Train Epoch: 2 [240/601 (39%)]\tLoss: 1.005833\n",
      "Train Epoch: 2 [320/601 (53%)]\tLoss: 0.768189\n",
      "Train Epoch: 2 [400/601 (66%)]\tLoss: 0.969904\n",
      "Train Epoch: 2 [480/601 (79%)]\tLoss: 1.174253\n",
      "Train Epoch: 2 [560/601 (92%)]\tLoss: 0.907876\n",
      "Epoch: 2, Training Accuracy: 65.89%\n",
      "Epoch: 2, Validation Accuracy: 46.15%\n",
      "Epoch: 2, Testing Accuracy: 53.85%\n",
      "Train Epoch: 3 [0/601 (0%)]\tLoss: 1.433683\n",
      "Train Epoch: 3 [80/601 (13%)]\tLoss: 0.929222\n",
      "Train Epoch: 3 [160/601 (26%)]\tLoss: 0.741597\n",
      "Train Epoch: 3 [240/601 (39%)]\tLoss: 0.768387\n",
      "Train Epoch: 3 [320/601 (53%)]\tLoss: 0.577292\n",
      "Train Epoch: 3 [400/601 (66%)]\tLoss: 0.605998\n",
      "Train Epoch: 3 [480/601 (79%)]\tLoss: 0.838661\n",
      "Train Epoch: 3 [560/601 (92%)]\tLoss: 0.673520\n",
      "Epoch: 3, Training Accuracy: 73.71%\n",
      "Epoch: 3, Validation Accuracy: 60.26%\n",
      "Epoch: 3, Testing Accuracy: 62.82%\n",
      "Train Epoch: 4 [0/601 (0%)]\tLoss: 0.860041\n",
      "Train Epoch: 4 [80/601 (13%)]\tLoss: 0.638486\n",
      "Train Epoch: 4 [160/601 (26%)]\tLoss: 0.782398\n",
      "Train Epoch: 4 [240/601 (39%)]\tLoss: 0.708211\n",
      "Train Epoch: 4 [320/601 (53%)]\tLoss: 0.695983\n",
      "Train Epoch: 4 [400/601 (66%)]\tLoss: 0.612340\n",
      "Train Epoch: 4 [480/601 (79%)]\tLoss: 0.572188\n",
      "Train Epoch: 4 [560/601 (92%)]\tLoss: 0.474196\n",
      "Epoch: 4, Training Accuracy: 73.54%\n",
      "Epoch: 4, Validation Accuracy: 60.26%\n",
      "Epoch: 4, Testing Accuracy: 66.67%\n",
      "Train Epoch: 5 [0/601 (0%)]\tLoss: 0.701929\n",
      "Train Epoch: 5 [80/601 (13%)]\tLoss: 0.677765\n",
      "Train Epoch: 5 [160/601 (26%)]\tLoss: 0.439566\n",
      "Train Epoch: 5 [240/601 (39%)]\tLoss: 0.469740\n",
      "Train Epoch: 5 [320/601 (53%)]\tLoss: 0.661414\n",
      "Train Epoch: 5 [400/601 (66%)]\tLoss: 0.665062\n",
      "Train Epoch: 5 [480/601 (79%)]\tLoss: 0.500653\n",
      "Train Epoch: 5 [560/601 (92%)]\tLoss: 0.565797\n",
      "Epoch: 5, Training Accuracy: 84.03%\n",
      "Epoch: 5, Validation Accuracy: 70.51%\n",
      "Epoch: 5, Testing Accuracy: 71.79%\n",
      "Train Epoch: 6 [0/601 (0%)]\tLoss: 0.465449\n",
      "Train Epoch: 6 [80/601 (13%)]\tLoss: 0.646992\n",
      "Train Epoch: 6 [160/601 (26%)]\tLoss: 0.688365\n",
      "Train Epoch: 6 [240/601 (39%)]\tLoss: 0.561333\n",
      "Train Epoch: 6 [320/601 (53%)]\tLoss: 0.347341\n",
      "Train Epoch: 6 [400/601 (66%)]\tLoss: 0.717414\n",
      "Train Epoch: 6 [480/601 (79%)]\tLoss: 0.382617\n",
      "Train Epoch: 6 [560/601 (92%)]\tLoss: 0.376734\n",
      "Epoch: 6, Training Accuracy: 76.37%\n",
      "Epoch: 6, Validation Accuracy: 64.10%\n",
      "Epoch: 6, Testing Accuracy: 69.23%\n",
      "Train Epoch: 7 [0/601 (0%)]\tLoss: 0.812771\n",
      "Train Epoch: 7 [80/601 (13%)]\tLoss: 0.659971\n",
      "Train Epoch: 7 [160/601 (26%)]\tLoss: 0.302415\n",
      "Train Epoch: 7 [240/601 (39%)]\tLoss: 0.612095\n",
      "Train Epoch: 7 [320/601 (53%)]\tLoss: 0.777581\n",
      "Train Epoch: 7 [400/601 (66%)]\tLoss: 0.304952\n",
      "Train Epoch: 7 [480/601 (79%)]\tLoss: 0.420199\n",
      "Train Epoch: 7 [560/601 (92%)]\tLoss: 0.372838\n",
      "Epoch: 7, Training Accuracy: 85.36%\n",
      "Epoch: 7, Validation Accuracy: 73.08%\n",
      "Epoch: 7, Testing Accuracy: 78.21%\n",
      "Train Epoch: 8 [0/601 (0%)]\tLoss: 0.384444\n",
      "Train Epoch: 8 [80/601 (13%)]\tLoss: 0.458372\n",
      "Train Epoch: 8 [160/601 (26%)]\tLoss: 0.322779\n",
      "Train Epoch: 8 [240/601 (39%)]\tLoss: 0.285476\n",
      "Train Epoch: 8 [320/601 (53%)]\tLoss: 0.485816\n",
      "Train Epoch: 8 [400/601 (66%)]\tLoss: 0.430263\n",
      "Train Epoch: 8 [480/601 (79%)]\tLoss: 0.433296\n",
      "Train Epoch: 8 [560/601 (92%)]\tLoss: 0.292190\n",
      "Epoch: 8, Training Accuracy: 80.03%\n",
      "Epoch: 8, Validation Accuracy: 71.79%\n",
      "Epoch: 8, Testing Accuracy: 74.36%\n",
      "Train Epoch: 9 [0/601 (0%)]\tLoss: 0.372619\n",
      "Train Epoch: 9 [80/601 (13%)]\tLoss: 0.378448\n",
      "Train Epoch: 9 [160/601 (26%)]\tLoss: 0.530941\n",
      "Train Epoch: 9 [240/601 (39%)]\tLoss: 0.945543\n",
      "Train Epoch: 9 [320/601 (53%)]\tLoss: 0.548463\n",
      "Train Epoch: 9 [400/601 (66%)]\tLoss: 0.665617\n",
      "Train Epoch: 9 [480/601 (79%)]\tLoss: 0.375022\n",
      "Train Epoch: 9 [560/601 (92%)]\tLoss: 0.532861\n",
      "Epoch: 9, Training Accuracy: 88.69%\n",
      "Epoch: 9, Validation Accuracy: 75.64%\n",
      "Epoch: 9, Testing Accuracy: 78.21%\n",
      "Train Epoch: 10 [0/601 (0%)]\tLoss: 0.270023\n",
      "Train Epoch: 10 [80/601 (13%)]\tLoss: 0.387555\n",
      "Train Epoch: 10 [160/601 (26%)]\tLoss: 0.342226\n",
      "Train Epoch: 10 [240/601 (39%)]\tLoss: 0.463479\n",
      "Train Epoch: 10 [320/601 (53%)]\tLoss: 0.401276\n",
      "Train Epoch: 10 [400/601 (66%)]\tLoss: 0.258320\n",
      "Train Epoch: 10 [480/601 (79%)]\tLoss: 0.348407\n",
      "Train Epoch: 10 [560/601 (92%)]\tLoss: 0.407646\n",
      "Epoch: 10, Training Accuracy: 86.19%\n",
      "Epoch: 10, Validation Accuracy: 76.92%\n",
      "Epoch: 10, Testing Accuracy: 75.64%\n",
      "Train Epoch: 11 [0/601 (0%)]\tLoss: 0.366594\n",
      "Train Epoch: 11 [80/601 (13%)]\tLoss: 0.106425\n",
      "Train Epoch: 11 [160/601 (26%)]\tLoss: 0.214730\n",
      "Train Epoch: 11 [240/601 (39%)]\tLoss: 0.390611\n",
      "Train Epoch: 11 [320/601 (53%)]\tLoss: 0.317440\n",
      "Train Epoch: 11 [400/601 (66%)]\tLoss: 0.241506\n",
      "Train Epoch: 11 [480/601 (79%)]\tLoss: 0.199296\n",
      "Train Epoch: 11 [560/601 (92%)]\tLoss: 0.532689\n",
      "Epoch: 11, Training Accuracy: 86.69%\n",
      "Epoch: 11, Validation Accuracy: 74.36%\n",
      "Epoch: 11, Testing Accuracy: 76.92%\n",
      "Train Epoch: 12 [0/601 (0%)]\tLoss: 0.415967\n",
      "Train Epoch: 12 [80/601 (13%)]\tLoss: 0.777556\n",
      "Train Epoch: 12 [160/601 (26%)]\tLoss: 0.381165\n",
      "Train Epoch: 12 [240/601 (39%)]\tLoss: 0.494189\n",
      "Train Epoch: 12 [320/601 (53%)]\tLoss: 0.483738\n",
      "Train Epoch: 12 [400/601 (66%)]\tLoss: 0.364097\n",
      "Train Epoch: 12 [480/601 (79%)]\tLoss: 0.319566\n",
      "Train Epoch: 12 [560/601 (92%)]\tLoss: 0.341102\n",
      "Epoch: 12, Training Accuracy: 91.51%\n",
      "Epoch: 12, Validation Accuracy: 74.36%\n",
      "Epoch: 12, Testing Accuracy: 78.21%\n",
      "Train Epoch: 13 [0/601 (0%)]\tLoss: 0.158057\n",
      "Train Epoch: 13 [80/601 (13%)]\tLoss: 0.191355\n",
      "Train Epoch: 13 [160/601 (26%)]\tLoss: 0.372170\n",
      "Train Epoch: 13 [240/601 (39%)]\tLoss: 0.090590\n",
      "Train Epoch: 13 [320/601 (53%)]\tLoss: 0.391271\n",
      "Train Epoch: 13 [400/601 (66%)]\tLoss: 0.100473\n",
      "Train Epoch: 13 [480/601 (79%)]\tLoss: 0.642220\n",
      "Train Epoch: 13 [560/601 (92%)]\tLoss: 0.476498\n",
      "Epoch: 13, Training Accuracy: 90.85%\n",
      "Epoch: 13, Validation Accuracy: 74.36%\n",
      "Epoch: 13, Testing Accuracy: 80.77%\n",
      "Train Epoch: 14 [0/601 (0%)]\tLoss: 0.389736\n",
      "Train Epoch: 14 [80/601 (13%)]\tLoss: 0.687081\n",
      "Train Epoch: 14 [160/601 (26%)]\tLoss: 0.238565\n",
      "Train Epoch: 14 [240/601 (39%)]\tLoss: 0.307830\n",
      "Train Epoch: 14 [320/601 (53%)]\tLoss: 0.090604\n",
      "Train Epoch: 14 [400/601 (66%)]\tLoss: 0.393133\n",
      "Train Epoch: 14 [480/601 (79%)]\tLoss: 0.365478\n",
      "Train Epoch: 14 [560/601 (92%)]\tLoss: 0.307102\n",
      "Epoch: 14, Training Accuracy: 84.53%\n",
      "Epoch: 14, Validation Accuracy: 69.23%\n",
      "Epoch: 14, Testing Accuracy: 76.92%\n",
      "Train Epoch: 15 [0/601 (0%)]\tLoss: 0.213183\n",
      "Train Epoch: 15 [80/601 (13%)]\tLoss: 0.479593\n",
      "Train Epoch: 15 [160/601 (26%)]\tLoss: 0.408224\n",
      "Train Epoch: 15 [240/601 (39%)]\tLoss: 0.406748\n",
      "Train Epoch: 15 [320/601 (53%)]\tLoss: 0.393325\n",
      "Train Epoch: 15 [400/601 (66%)]\tLoss: 0.198312\n",
      "Train Epoch: 15 [480/601 (79%)]\tLoss: 0.197277\n",
      "Train Epoch: 15 [560/601 (92%)]\tLoss: 0.104172\n",
      "Epoch: 15, Training Accuracy: 69.05%\n",
      "Epoch: 15, Validation Accuracy: 61.54%\n",
      "Epoch: 15, Testing Accuracy: 71.79%\n",
      "Train Epoch: 16 [0/601 (0%)]\tLoss: 0.463229\n",
      "Train Epoch: 16 [80/601 (13%)]\tLoss: 0.464216\n",
      "Train Epoch: 16 [160/601 (26%)]\tLoss: 0.104413\n",
      "Train Epoch: 16 [240/601 (39%)]\tLoss: 0.610738\n",
      "Train Epoch: 16 [320/601 (53%)]\tLoss: 0.497032\n",
      "Train Epoch: 16 [400/601 (66%)]\tLoss: 0.298327\n",
      "Train Epoch: 16 [480/601 (79%)]\tLoss: 0.587443\n",
      "Train Epoch: 16 [560/601 (92%)]\tLoss: 0.350646\n",
      "Epoch: 16, Training Accuracy: 93.51%\n",
      "Epoch: 16, Validation Accuracy: 78.21%\n",
      "Epoch: 16, Testing Accuracy: 83.33%\n",
      "Train Epoch: 17 [0/601 (0%)]\tLoss: 0.324011\n",
      "Train Epoch: 17 [80/601 (13%)]\tLoss: 0.347912\n",
      "Train Epoch: 17 [160/601 (26%)]\tLoss: 0.262090\n",
      "Train Epoch: 17 [240/601 (39%)]\tLoss: 0.128283\n",
      "Train Epoch: 17 [320/601 (53%)]\tLoss: 0.169471\n",
      "Train Epoch: 17 [400/601 (66%)]\tLoss: 0.265714\n",
      "Train Epoch: 17 [480/601 (79%)]\tLoss: 0.483201\n",
      "Train Epoch: 17 [560/601 (92%)]\tLoss: 0.448768\n",
      "Epoch: 17, Training Accuracy: 93.34%\n",
      "Epoch: 17, Validation Accuracy: 78.21%\n",
      "Epoch: 17, Testing Accuracy: 84.62%\n",
      "Train Epoch: 18 [0/601 (0%)]\tLoss: 0.222096\n",
      "Train Epoch: 18 [80/601 (13%)]\tLoss: 0.318448\n",
      "Train Epoch: 18 [160/601 (26%)]\tLoss: 0.271826\n",
      "Train Epoch: 18 [240/601 (39%)]\tLoss: 0.522378\n",
      "Train Epoch: 18 [320/601 (53%)]\tLoss: 0.152368\n",
      "Train Epoch: 18 [400/601 (66%)]\tLoss: 0.150629\n",
      "Train Epoch: 18 [480/601 (79%)]\tLoss: 0.453090\n",
      "Train Epoch: 18 [560/601 (92%)]\tLoss: 0.266108\n",
      "Epoch: 18, Training Accuracy: 89.18%\n",
      "Epoch: 18, Validation Accuracy: 76.92%\n",
      "Epoch: 18, Testing Accuracy: 83.33%\n",
      "Train Epoch: 19 [0/601 (0%)]\tLoss: 0.212998\n",
      "Train Epoch: 19 [80/601 (13%)]\tLoss: 0.266413\n",
      "Train Epoch: 19 [160/601 (26%)]\tLoss: 0.162876\n",
      "Train Epoch: 19 [240/601 (39%)]\tLoss: 0.205774\n",
      "Train Epoch: 19 [320/601 (53%)]\tLoss: 0.171455\n",
      "Train Epoch: 19 [400/601 (66%)]\tLoss: 0.229365\n",
      "Train Epoch: 19 [480/601 (79%)]\tLoss: 0.123933\n",
      "Train Epoch: 19 [560/601 (92%)]\tLoss: 0.371013\n",
      "Epoch: 19, Training Accuracy: 91.68%\n",
      "Epoch: 19, Validation Accuracy: 82.05%\n",
      "Epoch: 19, Testing Accuracy: 83.33%\n",
      "Train Epoch: 20 [0/601 (0%)]\tLoss: 0.131496\n",
      "Train Epoch: 20 [80/601 (13%)]\tLoss: 0.456421\n",
      "Train Epoch: 20 [160/601 (26%)]\tLoss: 0.347288\n",
      "Train Epoch: 20 [240/601 (39%)]\tLoss: 0.138727\n",
      "Train Epoch: 20 [320/601 (53%)]\tLoss: 0.357895\n",
      "Train Epoch: 20 [400/601 (66%)]\tLoss: 0.131830\n",
      "Train Epoch: 20 [480/601 (79%)]\tLoss: 0.300872\n",
      "Train Epoch: 20 [560/601 (92%)]\tLoss: 0.195941\n",
      "Epoch: 20, Training Accuracy: 93.18%\n",
      "Epoch: 20, Validation Accuracy: 76.92%\n",
      "Epoch: 20, Testing Accuracy: 88.46%\n",
      "Training and evaluation finished in: 18.905198097229004 sec.\n",
      "Final Training Loss (Last Epoch): 0.216405\n",
      "Final Evaluation Metrics:\n",
      "Precision: 74.54%, Recall: 74.81%, F1-Score: 74.31%\n",
      "Confusion Matrix:\n",
      "[[254   4  39  43]\n",
      " [ 81 205   9  85]\n",
      " [ 18   3 397   2]\n",
      " [ 24  78   7 311]]\n",
      "Class 0 Accuracy: 74.71%\n",
      "Class 1 Accuracy: 53.95%\n",
      "Class 2 Accuracy: 94.52%\n",
      "Class 3 Accuracy: 74.05%\n"
     ]
    }
   ],
   "source": [
    "# Uncomment following line to change the learning rate\n",
    "learning_rate = 0.01\n",
    "num_hidden_unit3 = 25\n",
    "# 5.1 Instantiate from the model class\n",
    "model = FourLayerNN(image_size, \n",
    "                   num_classes, \n",
    "                   num_hidden_unit1,\n",
    "                   num_hidden_unit2,\n",
    "                   num_hidden_unit3\n",
    "                  )\n",
    "\n",
    "# for running on gpu\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# 5.2 Initialize model's weight    \n",
    "model.apply(weights_init)\n",
    "\n",
    "# 5.3 Define optimizer and loss function\n",
    "optimizer = optim.SGD(params = model.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 5.4 Training loop\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "total_time = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "train_losses = []\n",
    "for epoch in range(itr):\n",
    "    start = time()\n",
    "    tr_acc, final_loss = train_model(model, optimizer, train_loader, loss_func, epoch+1)\n",
    "    train_losses.append(final_loss)\n",
    "    vs_acc = eval_model_val(model, val_loader, epoch+1)\n",
    "    ts_acc, epoch_preds, epoch_labels = eval_model_test(model, test_loader,epoch+1)\n",
    "    train_acc.append(tr_acc)\n",
    "    test_acc.append(ts_acc)\n",
    "    all_preds.extend(epoch_preds)\n",
    "    all_labels.extend(epoch_labels)\n",
    "    end = time()\n",
    "    total_time += end-start\n",
    "print(\"Training and evaluation finished in:\", total_time, \"sec.\")\n",
    "print(f\"Final Training Loss (Last Epoch): {train_losses[-1]:.6f}\")\n",
    "final_evaluation(all_preds, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365065d-8320-4758-b1d6-290031f0bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
